{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c38596",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "There are several ways to interact with Spark SQL including SQL and the Dataset API. **When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation.** This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e7b30",
   "metadata": {},
   "source": [
    "## Caching\n",
    "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001b89f",
   "metadata": {},
   "source": [
    "If you are building a packaged PySpark application or library you can add it to your **setup.py** file as:\n",
    "\n",
    "```\n",
    "    install_requires=[\n",
    "        'pyspark==3.3.0'\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97514f2",
   "metadata": {},
   "source": [
    "### Python code examples in Spark Repo\n",
    "- https://github.com/apache/spark/tree/master/examples/src/main/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45b6ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7dff276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b1076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 20:51:47 WARN Utils: Your hostname, Pauls-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.90 instead (on interface en0)\n",
      "22/09/24 20:51:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 20:51:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Lines with a: 425, lines with b: 350\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SimpleApp.py\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logFile = \"/Users/devos/github/python-notes/python-libraries.md\"  # Should be some file on your system\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "logData = spark.read.text(logFile).cache()\n",
    "\n",
    "numAs = logData.filter(logData.value.contains('a')).count()\n",
    "numBs = logData.filter(logData.value.contains('b')).count()\n",
    "\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae45938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(logData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c48de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile = \"/Users/devos/github/python-notes/python-libraries.md\"  # Should be some file on your system\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "logData = spark.read.text(logFile).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34dded57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# Python Resources')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1ae6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|  # Python Resources|\n",
      "|                    |\n",
      "|1. Installing and...|\n",
      "|   - [pyenv](http...|\n",
      "|2. Managing virtu...|\n",
      "|   - [pyenv-virtu...|\n",
      "|   - [venv](https...|\n",
      "|             - conda|\n",
      "|      3. Code Editor|\n",
      "|                    |\n",
      "|   - I recommend ...|\n",
      "|   - To get set u...|\n",
      "|     - [YouTube: ...|\n",
      "|     - [YouTube: ...|\n",
      "|     - [Miguel Gr...|\n",
      "|   - VS Code Plui...|\n",
      "|      - EditorConfig|\n",
      "|                    |\n",
      "|     - Generate ....|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a4172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logData.filter(logData['value'].contains('z')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c7f598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/24 20:57:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "22/09/24 20:57:53 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/09/24 20:57:53 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "Coefficients: [-0.4963068060199876,0.19844393975928495]\n",
      "Intercept: 2.638090563156022\n",
      "Scale: 1.5472326865488453\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "|label|censor|features      |prediction        |quantiles                              |\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "|1.218|1.0   |[1.56,-0.605] |5.7189965530298865|[1.1603295951029065,4.995471733719635] |\n",
      "|2.949|0.0   |[0.346,2.158] |18.076458028588952|[3.6675401061563977,15.789559285491242]|\n",
      "|3.627|0.0   |[1.38,0.231]  |7.381875365763494 |[1.4977117707333778,6.447975512763019] |\n",
      "|0.273|1.0   |[0.52,1.151]  |13.577581299077908|[2.754761130759775,11.859846908963428] |\n",
      "|4.199|0.0   |[0.795,-0.226]|9.013093216625721 |[1.8286702406091526,7.872823838856871] |\n",
      "+-----+------+--------------+------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# $example on$\n",
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# $example off$\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"AFTSurvivalRegressionExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# $example on$\n",
    "training = spark.createDataFrame([\n",
    "    (1.218, 1.0, Vectors.dense(1.560, -0.605)),\n",
    "    (2.949, 0.0, Vectors.dense(0.346, 2.158)),\n",
    "    (3.627, 0.0, Vectors.dense(1.380, 0.231)),\n",
    "    (0.273, 1.0, Vectors.dense(0.520, 1.151)),\n",
    "    (4.199, 0.0, Vectors.dense(0.795, -0.226))], [\"label\", \"censor\", \"features\"])\n",
    "quantileProbabilities = [0.3, 0.6]\n",
    "aft = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities,\n",
    "                            quantilesCol=\"quantiles\")\n",
    "\n",
    "model = aft.fit(training)\n",
    "\n",
    "# Print the coefficients, intercept and scale parameter for AFT survival regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "print(\"Scale: \" + str(model.scale))\n",
    "model.transform(training).show(truncate=False)\n",
    "# $example off$\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e02a1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mMETA-INF\u001b[m\u001b[m/       full_user.avsc  people.json     users.avro\r\n",
      "\u001b[1m\u001b[36mdir1\u001b[m\u001b[m/           kv1.txt         people.txt      users.orc\r\n",
      "employees.json  people.csv      user.avsc       users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls ../resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9df92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fd1ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"../resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9c02054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e54cb261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age', 'name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15760e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e164079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Upper_Names|\n",
      "+-----------+\n",
      "|    MICHAEL|\n",
      "|       ANDY|\n",
      "|     JUSTIN|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.upper('name').alias('Upper_Names')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee18ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|Upper_Names|(age + 13)|\n",
      "+-----------+----------+\n",
      "|    MICHAEL|      null|\n",
      "|       ANDY|        43|\n",
      "|     JUSTIN|        32|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.upper('name').alias('Upper_Names'), F.column('age') + 13).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4464493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.column('age') > 22).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd6739",
   "metadata": {},
   "source": [
    "### Can use SQL syntax too\n",
    "- Register the DataFrame as a SQL Temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "012975c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "539b59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3043fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd44efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('../resources/people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "905dd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = lines.map(lambda s: s.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e6f014a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc9d79ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael', 'Andy', 'Justin']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts.map(lambda s: s[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b8ac13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Michael', age=29),\n",
       " Row(name='Andy', age=30),\n",
       " Row(name='Justin', age=19)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts.map(lambda p: Row(name=p[0], age=int(p[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019a2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3.3 (Python 3.9)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
